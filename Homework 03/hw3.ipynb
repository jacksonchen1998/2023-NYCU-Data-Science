{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "with open(\"./train.pkl\", \"rb\") as f:\n",
    "    train = pickle.load(f) # a dictionary\n",
    "\n",
    "with open(\"./validation.pkl\", \"rb\") as f:\n",
    "    val = pickle.load(f) # a dictionary\n",
    "    \n",
    "with open(\"./test.pkl\", \"rb\") as f:\n",
    "    test = pickle.load(f) # a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size 38400\n",
      "Validation dataset size 9600\n",
      "Test dataset size 600\n"
     ]
    }
   ],
   "source": [
    "# show the size of the train, val, test\n",
    "print(\"Train dataset size\", len(train['images']))\n",
    "print(\"Validation dataset size\", len(val['images']))\n",
    "print(\"Test dataset size\", len(test['qry_images']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New train dataset size 48000\n",
      "New train dataset shape (48000, 3, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "# numpy comcatenate from train and val\n",
    "new_train = {}\n",
    "new_train['images'] = np.concatenate((train['images'], val['images']), axis=0)\n",
    "new_train['labels'] = np.concatenate((train['labels'], val['labels']), axis=0)\n",
    "print(\"New train dataset size\", len(new_train['images']))\n",
    "print(\"New train dataset shape\", new_train['images'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New train label size 48000\n",
      "New train label shape 48000\n"
     ]
    }
   ],
   "source": [
    "print(\"New train label size\", len(new_train['labels']))\n",
    "print(\"New train label shape\", new_train['labels'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class number 64\n",
      "Class number {0: 1200, 1: 1200, 2: 1200, 3: 1200, 4: 1200, 5: 1200, 6: 1200, 7: 1200, 8: 1200, 9: 1200, 10: 1200, 11: 1200, 12: 1200, 13: 1200, 14: 1200, 15: 1200, 16: 600, 17: 600, 18: 600, 19: 600, 20: 600, 21: 600, 22: 600, 23: 600, 24: 600, 25: 600, 26: 600, 27: 600, 28: 600, 29: 600, 30: 600, 31: 600, 32: 600, 33: 600, 34: 600, 35: 600, 36: 600, 37: 600, 38: 600, 39: 600, 40: 600, 41: 600, 42: 600, 43: 600, 44: 600, 45: 600, 46: 600, 47: 600, 48: 600, 49: 600, 50: 600, 51: 600, 52: 600, 53: 600, 54: 600, 55: 600, 56: 600, 57: 600, 58: 600, 59: 600, 60: 600, 61: 600, 62: 600, 63: 600}\n"
     ]
    }
   ],
   "source": [
    "# traversal all new_train['labels'] and calculate the number of each class\n",
    "class_num = {}\n",
    "for i in range(new_train['labels'].shape[0]):\n",
    "    if new_train['labels'][i] in class_num:\n",
    "        class_num[new_train['labels'][i]] += 1\n",
    "    else:\n",
    "        class_num[new_train['labels'][i]] = 1\n",
    "print(\"Class number\", len(class_num))\n",
    "print(\"Class number\", class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = torchvision.transforms.ToPILImage()\n",
    "# img2 = transform(torch.tensor(new_train[\"images\"][38400])*255)\n",
    "# img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(new_train['images']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# data augmentation for train\n",
    "\n",
    "# set device as gpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "        [transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=15),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "val_transform = transforms.Compose(\n",
    "        [transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, images, labels, transform=None):\n",
    "                # load data from numpy array\n",
    "                self.images = torch.from_numpy(images)\n",
    "                self.labels = labels\n",
    "                self.transform = transform\n",
    "                \n",
    "        def __len__(self):\n",
    "                return len(self.images)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "                image = self.images[idx]\n",
    "                label = self.labels[idx]\n",
    "                if self.transform:\n",
    "                        image = self.transform(image)\n",
    "                return image, label\n",
    "\n",
    "# doing data augmentation for train[\"images\"]\n",
    "train_dataset = CustomDataset(new_train[\"images\"], new_train[\"labels\"], transform=train_transform)\n",
    "val_dataset = CustomDataset(val[\"images\"], val[\"labels\"], transform=val_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=8)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader size 750\n",
      "Validation loader size 150\n"
     ]
    }
   ],
   "source": [
    "# show the size of the train_loader\n",
    "print(\"Train loader size\", len(train_loader))\n",
    "print(\"Validation loader size\", len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(out_channels)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels, out_channels, 1, stride=stride),\n",
    "                torch.nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        if self.downsample is not None: # identity mapping used\n",
    "            identity = self.downsample(identity)\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ResNet18(torch.nn.Module):\n",
    "    def __init__(self, num_classes=80):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(64)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.layer1 = self._make_layer(64, 64, 2)\n",
    "        self.layer2 = self._make_layer(64, 128, 2, stride=2) \n",
    "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
    "        self.avgpool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = torch.nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, block_num, stride=1):\n",
    "        layers = []\n",
    "        layers.append(ResNetBlock(in_channels, out_channels, stride=stride))\n",
    "        for i in range(1, block_num):\n",
    "            layers.append(ResNetBlock(out_channels, out_channels))\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model with train_loader and test the model with val_loader\n",
    "\n",
    "model = ResNet18().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1280, gamma=0.8) # step_size: how many epochs to decay the learning rate by gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Epoch [3/100]:  78%|███████▊  | 583/750 [02:14<00:39,  4.27it/s, acc=0.0944, loss=0.0246]"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train_epoch = 100\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "val_acc = []\n",
    "val_loss = []\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(train_epoch):\n",
    "    train_loss_sum = 0\n",
    "    train_acc_sum = 0\n",
    "    \n",
    "    model.train()\n",
    "    train_loop = tqdm.tqdm((train_loader), total=len(train_loader), leave=False)\n",
    "    for batch_idx, (data, target) in enumerate(train_loop):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.to(device))\n",
    "        loss = criterion(output, target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_sum += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        train_acc_sum += (predicted == target.to(device)).sum().item()\n",
    "        train_loop.set_description(f'Training, Epoch [{epoch + 1}/{train_epoch}]')\n",
    "        train_loop.set_postfix(loss=train_loss_sum / ((batch_idx + 1) * batch_size), acc=train_acc_sum / ((batch_idx + 1) * batch_size))\n",
    "\n",
    "    model.eval()\n",
    "    val_loss_sum = 0\n",
    "    val_acc_sum = 0\n",
    "    test_loop = tqdm.tqdm((val_loader), total=len(val_loader), leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loop):\n",
    "            output = model(data.to(device))\n",
    "            loss = criterion(output, target.to(device))\n",
    "            val_loss_sum += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            val_acc_sum += (predicted == target.to(device)).sum().item()\n",
    "            test_loop.set_description(f'Testing, Epoch [{epoch + 1}/{train_epoch}]')\n",
    "            test_loop.set_postfix(loss=val_loss_sum / ((batch_idx + 1) * batch_size), acc=val_acc_sum / ((batch_idx + 1) * batch_size))\n",
    "\n",
    "        val_acc.append(val_acc_sum / len(val_loader.dataset))\n",
    "        val_loss.append(val_loss_sum / len(val_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot the loss and accuracy\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(train_loss, label='train')\n",
    "plt.plot(val_loss, label='validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(train_acc, label='train')\n",
    "plt.plot(val_acc, label='validation')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
