{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image from train folder\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# load file from train folder\n",
    "def load_file(path):\n",
    "    file_list = []\n",
    "    for file in os.listdir(path):\n",
    "        file_list.append(file)\n",
    "    return file_list\n",
    "\n",
    "train_data = load_file('train')\n",
    "\n",
    "# remove npy file format from train_data\n",
    "train_image = [x for x in train_data if x.endswith(\".npy\") == False]\n",
    "\n",
    "# sort train_image\n",
    "train_image.sort()\n",
    "\n",
    "train_label = [x for x in train_data if x.endswith(\".npy\") == True]\n",
    "train_label.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch image generator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "train_transform = transforms.Compose([transforms.Resize((256, 256)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.RandomAffine(30, translate=(0.2, 0.2), scale=(0.8, 1.2), shear=30),\n",
    "                                        transforms.RandomHorizontalFlip(p=0.3),\n",
    "                                        transforms.RandomCrop(256, pad_if_needed=True),\n",
    "                                        transforms.Grayscale(num_output_channels=1), # turn image to 1 channel\n",
    "                                        # turn image to 1 channel\n",
    "                                        transforms.Normalize([0.5], [0.5]), # normalize image to [-1, 1]\n",
    "                                        transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "                                        ])\n",
    "\n",
    "# create custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.file_list = []\n",
    "        for file in os.listdir(self.path):\n",
    "            # remove npy file format from train_data\n",
    "            if file.endswith(\".npy\") == False:\n",
    "                self.file_list.append(file)\n",
    "        self.file_list.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.path + self.file_list[idx])\n",
    "        image = self.transform(image)\n",
    "        return image\n",
    "    \n",
    "class Ground_Truth(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.file_list = []\n",
    "        self.image_list = []\n",
    "        for file in os.listdir(self.path):\n",
    "            # remove npy file format from train_data\n",
    "            if file.endswith(\".npy\") == True:\n",
    "                self.file_list.append(file)\n",
    "            if file.endswith(\".npy\") == False:\n",
    "                self.image_list.append(file)\n",
    "        self.file_list.sort()\n",
    "        self.image_list.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label = np.load(self.path + self.file_list[idx])\n",
    "        # get image size\n",
    "        image = Image.open(self.path + self.image_list[idx])\n",
    "        width, height = image.size\n",
    "        label_array = np.zeros((1, height, width))\n",
    "        #print(label_array.shape)\n",
    "        for i in label:\n",
    "            # label dot 1 aand also check if the label is out of image size\n",
    "            if i[0] < width and i[1] < height:\n",
    "                label_array[0][int(i[1])][int(i[0])] = 1\n",
    "            # add gaussian blur\n",
    "        # numpy array to image and 1 channel\n",
    "        array_image = Image.fromarray(label_array[0])\n",
    "        array_image = self.transform(array_image)\n",
    "        # print(\"People: \", np.sum(label_array))\n",
    "        return array_image\n",
    "    \n",
    "# create custom dataset from train_image list\n",
    "custom_dataset = CustomDataset('train/', transform=train_transform)\n",
    "\n",
    "# create ground truth dataset from train_label list\n",
    "ground_truth = Ground_Truth('train/', transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using custom_dataset and ground_truth to train CSRNet\n",
    "\n",
    "# create dataloader\n",
    "train_loader = DataLoader(custom_dataset, batch_size=64, shuffle=False)\n",
    "label_loader = DataLoader(ground_truth, batch_size=64, shuffle=False)\n",
    "\n",
    "# define device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision import\n",
    "import torchvision\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, in_channels, middle_channels, out_channels):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "    self.conv_relu = nn.Sequential(\n",
    "        nn.Conv2d(middle_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "        )\n",
    "  def forward(self, x1, x2):\n",
    "    x1 = self.up(x1)\n",
    "    x1 = torch.cat((x1, x2), dim=1)\n",
    "    x1 = self.conv_relu(x1)\n",
    "    return x1\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = torchvision.models.resnet18(True)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n",
    "            self.base_layers[1],\n",
    "            self.base_layers[2])\n",
    "        self.layer2 = nn.Sequential(*self.base_layers[3:5])\n",
    "        self.layer3 = self.base_layers[5]\n",
    "        self.layer4 = self.base_layers[6]\n",
    "        self.layer5 = self.base_layers[7]\n",
    "        self.decode4 = Decoder(512, 256+256, 256)\n",
    "        self.decode3 = Decoder(256, 256+128, 256)\n",
    "        self.decode2 = Decoder(256, 128+64, 128)\n",
    "        self.decode1 = Decoder(128, 64+64, 64)\n",
    "        self.decode0 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1, bias=False),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
    "            )\n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        e1 = self.layer1(input) # 64,128,128\n",
    "        e2 = self.layer2(e1) # 64,64,64\n",
    "        e3 = self.layer3(e2) # 128,32,32\n",
    "        e4 = self.layer4(e3) # 256,16,16\n",
    "        f = self.layer5(e4) # 512,8,8\n",
    "        d4 = self.decode4(f, e4) # 256,16,16\n",
    "        d3 = self.decode3(d4, e3) # 256,32,32\n",
    "        d2 = self.decode2(d3, e2) # 128,64,64\n",
    "        d1 = self.decode1(d2, e1) # 64,128,128\n",
    "        d0 = self.decode0(d1) # 64,256,256\n",
    "        out = self.conv_last(d0) # 1,256,256\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model to summary\n",
    "from torchsummary import summary\n",
    "\n",
    "# define model\n",
    "model = Unet(n_class=1).to(device)\n",
    "\n",
    "# summary model\n",
    "summary(model, input_size=(1, 256, 256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = train_loader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed image to model\n",
    "\n",
    "output = model(image.unsqueeze(0).to(device))\n",
    "\n",
    "print(output.squeeze(0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show output image\n",
    "\n",
    "plt.imshow(output.squeeze(0).permute(1, 2, 0).detach().cpu().numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show ground truth image\n",
    "\n",
    "print(label_loader.dataset[0].shape)\n",
    "plt.imshow(label_loader.dataset[0].permute(1, 2, 0).detach().cpu().numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bay_Loss(Module):\n",
    "    def __init__(self, use_background, device):\n",
    "        super(Bay_Loss, self).__init__()\n",
    "        self.device = device\n",
    "        self.use_bg = use_background\n",
    "\n",
    "    def forward(self, prob_list, target_list, pre_density):\n",
    "        loss = 0\n",
    "        for idx, prob in enumerate(prob_list):  # iterative through each sample\n",
    "            if prob is None:  # image contains no annotation points\n",
    "                pre_count = torch.sum(pre_density[idx])\n",
    "                target = torch.zeros((1,), dtype=torch.float32, device=self.device)\n",
    "            else:\n",
    "                N = len(prob)\n",
    "                if self.use_bg:\n",
    "                    target = torch.zeros((N,), dtype=torch.float32, device=self.device)\n",
    "                    target[:-1] = target_list[idx]\n",
    "                else:\n",
    "                    target = target_list[idx]\n",
    "                pre_count = torch.sum(pre_density[idx].view((1, -1)) * prob, dim=1)  # flatten into vector\n",
    "\n",
    "            loss += torch.sum(torch.abs(target - pre_count))\n",
    "        loss = loss / len(prob_list)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayesian loss for crowd counting with U-Net\n",
    "\n",
    "# define loss function\n",
    "class BayesianLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BayesianLoss, self).__init__()\n",
    "        self.loss = nn.MSELoss(reduction='mean')\n",
    "    \n",
    "    def forward(self, pred, gt, var):\n",
    "        # calculate loss\n",
    "        loss = torch.sum((pred - gt)**2 / var + torch.log(var))\n",
    "        return loss\n",
    "    \n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "# define loss function\n",
    "criterion = BayesianLoss()\n",
    "\n",
    "# scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=5, min_lr=1e-5, verbose=True)\n",
    "\n",
    "# define epoch\n",
    "epoch = 100\n",
    "\n",
    "# define list to store loss\n",
    "train_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "for i in range(epoch):\n",
    "    # set model to train mode\n",
    "    model.train()\n",
    "    # define loss\n",
    "    loss = 0\n",
    "    # iterate over train_loader\n",
    "    for image, label in zip(train_loader, label_loader):\n",
    "        # feed image to model\n",
    "        output = model(image.to(device))\n",
    "        # calculate loss\n",
    "        loss = criterion(output.squeeze(0), label.to(device), torch.ones_like(output.squeeze(0)))\n",
    "        optimizer.zero_grad()\n",
    "        # backward loss\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "    # append loss to train_loss\n",
    "    train_loss.append(loss.item())\n",
    "    # print loss\n",
    "    print(\"Epoch: {}, Loss: {}\".format(i, loss.item()))\n",
    "    # zero grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
