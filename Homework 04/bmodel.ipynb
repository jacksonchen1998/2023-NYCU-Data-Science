{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "# pytorch image generator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "# torchvision import\n",
    "import random\n",
    "from bay_loss import Bay_Loss\n",
    "from post_prob import Post_Prob\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(im_h, im_w, crop_h, crop_w):\n",
    "    res_h = im_h - crop_h\n",
    "    res_w = im_w - crop_w\n",
    "    i = random.randint(0, res_h)\n",
    "    j = random.randint(0, res_w)\n",
    "    return i, j, crop_h, crop_w\n",
    "\n",
    "def cal_innner_area(c_left, c_up, c_right, c_down, bbox):\n",
    "    inner_left = np.maximum(c_left, bbox[:, 0])\n",
    "    inner_up = np.maximum(c_up, bbox[:, 1])\n",
    "    inner_right = np.minimum(c_right, bbox[:, 2])\n",
    "    inner_down = np.minimum(c_down, bbox[:, 3])\n",
    "    inner_area = np.maximum(inner_right-inner_left, 0.0) * np.maximum(inner_down-inner_up, 0.0)\n",
    "    return inner_area\n",
    "\n",
    "class Crowd(Dataset):\n",
    "    def __init__(self, root_path, crop_size,\n",
    "                 downsample_ratio, is_gray=False,\n",
    "                 method='train'):\n",
    "        self.root_path = root_path\n",
    "        self.im_list = sorted(glob.glob(os.path.join(self.root_path, '*.jpg')))\n",
    "        if method not in ['train', 'test']:\n",
    "            raise Exception(\"not implement\")\n",
    "        self.method = method\n",
    "\n",
    "        self.c_size = crop_size\n",
    "        self.d_ratio = downsample_ratio\n",
    "        assert self.c_size % self.d_ratio == 0\n",
    "        self.dc_size = self.c_size // self.d_ratio\n",
    "\n",
    "        if is_gray:\n",
    "            self.trans = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "            ])\n",
    "        else:\n",
    "            self.trans = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.im_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img_path = self.im_list[item]\n",
    "        gd_path = img_path.replace('jpg', 'npy')\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.method == 'train':\n",
    "            keypoints = np.load(gd_path)\n",
    "            return self.train_transform(img, keypoints)\n",
    "        elif self.method == 'test':\n",
    "            keypoints = np.load(gd_path)\n",
    "            img = self.trans(img)\n",
    "            name = os.path.basename(img_path).split('.')[0]\n",
    "            return img, len(keypoints), name\n",
    "\n",
    "    def train_transform(self, img, keypoints):\n",
    "        \"\"\"random crop image patch and find people in it\"\"\"\n",
    "        wd, ht = img.size\n",
    "        st_size = min(wd, ht)\n",
    "        assert st_size >= self.c_size # assert the crop size is smaller than the original image\n",
    "        #assert len(keypoints) > 0 # assert there is at least one person in the image\n",
    "        i, j, h, w = random_crop(ht, wd, self.c_size, self.c_size)\n",
    "        img = transforms.functional.crop(img, i, j, h, w)\n",
    "        \n",
    "        #nearest_dis = np.clip(keypoints[:, 2], 4.0, 128.0)\n",
    "\n",
    "        # nearest_dis = np.minimum(keypoints[:, 0], keypoints[:, 1])\n",
    "        # nearest_dis = np.clip(nearest_dis, 0.0, st_size)\n",
    "\n",
    "        # points_left_up = keypoints[:, :2] - nearest_dis[:, None] / 2.0\n",
    "        # points_right_down = keypoints[:, :2] + nearest_dis[:, None] / 2.0\n",
    "        # bbox = np.concatenate((points_left_up, points_right_down), axis=1)\n",
    "        # inner_area = cal_innner_area(j, i, j+w, i+h, bbox)\n",
    "        # origin_area = nearest_dis * nearest_dis\n",
    "        # # ratio = np.clip(1.0 * inner_area / origin_area, 0.0, 1.0)\n",
    "        # mask = (ratio >= 0.5)\n",
    "        # keypoints = keypoints[mask]\n",
    "\n",
    "        if len(keypoints) > 0:\n",
    "            idx_mask = (keypoints[:, 0] >= j) * (keypoints[:, 0] <= j + 512) * (keypoints[:, 1] >= i) * (keypoints[:, 1] <= i + 512)\n",
    "            keypoints = keypoints[idx_mask]\n",
    "            keypoints = keypoints - [j, i]  # change coodinate\n",
    "        target = np.ones(len(keypoints))\n",
    "\n",
    "        if len(keypoints) > 0:\n",
    "            if random.random() > 0.5:\n",
    "                img = transforms.functional.hflip(img)\n",
    "                keypoints[:, 0] = w - keypoints[:, 0]\n",
    "        else:\n",
    "            if random.random() > 0.5:\n",
    "                img = transforms.functional.hflip(img)\n",
    "        return self.trans(img), torch.from_numpy(keypoints.copy()).float(), \\\n",
    "               torch.from_numpy(target.copy()).float(), st_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.reg_layer = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = F.upsample_bilinear(x, scale_factor=2)\n",
    "        x = self.reg_layer(x)\n",
    "        return torch.abs(x)\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "cfg = {\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512]\n",
    "}\n",
    "\n",
    "def vgg19():\n",
    "    \"\"\"VGG 19-layer model (configuration \"E\")\n",
    "        model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = VGG(make_layers(cfg['E']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = vgg19().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_workers = 4\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=5, min_lr=1e-5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_collate(batch):\n",
    "    transposed_batch = list(zip(*batch))\n",
    "    images = torch.stack(transposed_batch[0], 0)\n",
    "    points = transposed_batch[1]  # the number of points is not fixed, keep it as a list of tensor\n",
    "    targets = transposed_batch[2]\n",
    "    st_sizes = torch.FloatTensor(transposed_batch[3])\n",
    "    return images, points, targets, st_sizes\n",
    "\n",
    "datasets = {x: Crowd(os.path.join('./', x),\n",
    "                                  crop_size=512,\n",
    "                                  downsample_ratio=8,\n",
    "                                  is_gray=False) for x in ['train', 'test']}\n",
    "\n",
    "dataloaders = {x: DataLoader(datasets[x],\n",
    "                                          collate_fn=(train_collate\n",
    "                                                      if x == 'train' else default_collate),\n",
    "                                          batch_size=(batch_size\n",
    "                                          if x == 'train' else 1),\n",
    "                                          shuffle=(True if x == 'train' else False),\n",
    "                                          num_workers=num_workers,\n",
    "                                          pin_memory=(True if x == 'train' else False))\n",
    "                            for x in ['train', 'test']}\n",
    "dataloaders = {x: DataLoader(datasets[x], collate_fn=train_collate, batch_size=batch_size, shuffle=True, num_workers=num_workers) \n",
    "            for x in ['train', 'test']}\n",
    "dataset_sizes = {x: len(datasets[x]) for x in ['train', 'test']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_prob = Post_Prob(sigma=8.0, c_size=512, stride=8, background_ratio=1, use_background=True, device=device)\n",
    "criterion = Bay_Loss(use_background=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epoch = 1000\n",
    "best_loss = 1e6\n",
    "best_mae = 1e6\n",
    "best_mse = 1e6\n",
    "best_rmse = 1e6\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    temp_loss = 0.0\n",
    "    temp_mae = 0.0\n",
    "    temp_mse = 0.0\n",
    "    temp_rmse = 0.0\n",
    "    print('Epoch {}/{}'.format(epoch, total_epoch - 1))\n",
    "    print('-' * 40)\n",
    "    model.train()\n",
    "    for steps, (inputs, points, targets, st_sizes) in enumerate(dataloaders['train']):\n",
    "        inputs = inputs.to(device)\n",
    "        points = [point.to(device) for point in points]\n",
    "        targets = [target.to(device) for target in targets]\n",
    "        st_sizes = st_sizes.to(device)\n",
    "        gd_count = np.array([len(p) for p in points], dtype=np.float32)\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = model(inputs)\n",
    "            prob_list = post_prob(points, st_sizes)\n",
    "            loss = criterion(prob_list, targets, outputs)\n",
    "            temp_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            N = inputs.size(0)\n",
    "            pre_count = torch.sum(outputs.view(N, -1), dim=1).detach().cpu().numpy()\n",
    "            res = pre_count - gd_count\n",
    "            temp_mae += np.mean(np.fabs(res))\n",
    "            temp_mse += np.mean(res ** 2)\n",
    "            temp_rmse += np.sqrt(np.mean(res ** 2))\n",
    "\n",
    "    print('Best Loss: {:4f}, Current Loss: {:4f}'.format(best_loss, temp_loss / len(dataloaders['train'])))\n",
    "    print('Best MAE: {:4f}, Current MAE: {:4f}'.format(best_mae, temp_mae / len(dataloaders['train'])))\n",
    "    print('Best MSE: {:4f}, Current MSE: {:4f}'.format(best_mse, np.sqrt(temp_mse / len(dataloaders['train']))))\n",
    "    print('Best RMSE: {:4f}, Current RMSE: {:4f}'.format(best_rmse, temp_rmse / len(dataloaders['train'])))\n",
    "\n",
    "    if (temp_loss / len(dataloaders['train'])) < best_loss:\n",
    "        best_loss = temp_loss / len(dataloaders['train'])\n",
    "        best_mae = temp_mae / len(dataloaders['train'])\n",
    "        best_mse = np.sqrt(temp_mse / len(dataloaders['train']))\n",
    "        best_rmse = temp_rmse / len(dataloaders['train'])\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print('Model Saved!')\n",
    "    scheduler.step(loss.item())\n",
    "    print('-' * 40)\n",
    "    print()\n",
    "\n",
    "torch.save(model.state_dict(), 'final_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "pred = np.zeros(len(dataloaders['test'].dataset))\n",
    "\n",
    "for i, (img, target) in enumerate(dataloaders['test']):\n",
    "    img = img.to(device)\n",
    "    output = model(img)\n",
    "    pred[i] = output.detach().cpu().numpy().squeeze()\n",
    "    print('Index: {:d}, Pred: {:4f}, GT: {:4f}'.format(i, pred[i], target[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('result.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['ID', 'Count'])\n",
    "    for i in range(len(pred)):\n",
    "        writer.writerow([i+1, pred[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
